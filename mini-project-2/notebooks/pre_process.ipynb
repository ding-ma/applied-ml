{"cells":[{"cell_type":"markdown","source":"# Task 1: Acquire and preprocess the data\n* 20 news group dataset from _sklearn_\n* IMDB Reviews: located at root of this directory. GCS is too slow for a lot of files.","metadata":{"tags":[],"cell_id":"00000-42e0d603-bf08-4896-9f07-138c669376ca","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## some notes:\n* look at `spacy` package for preprocessing? we might be able to extract adjectives and give them more weights compared tot he rest of the words.\n* https://www.machinelearningplus.com/nlp/gensim-tutorial/\n* look at _n-grams_ (2 or 3 maybe useful)\n    * \"very\" might come up oftern in TFIDF and might be ignored. but if we look at 2-grams, \"very-good\" and \"very-bad\" provides very good insight on the rating of the moving and it should increase the TFIDF ranking\n* any processing that we want to do should be done in the `PreProcess` class so that it can be applied to both datasets\n* maybe look at phoneme of words?\n* fixing typos in review\n    * make words in singular and masculin \n\nspacy and nltk have different stopword library --> they are outputed in the dataset","metadata":{"tags":[],"cell_id":"00001-2a26447b-cbe0-4430-be2c-9f74c790a9e0","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-d287bc47-d4a4-44c7-a793-2cfdc6e1a529","deepnote_to_be_reexecuted":false,"source_hash":"7fcc8db0","execution_millis":9366,"execution_start":1613925964323,"deepnote_cell_type":"code"},"source":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.util import ngrams\n\ndataset_path = Path('dataset')\ndataset_path.mkdir(exist_ok=True, parents=True)\n\nnltk.download('stopwords')\n\nbigram_series = pd.read_html('dataset/bigrams.html')[1]\nbigram_series[0] = bigram_series[0].astype(str)\nbigram_series[0] = bigram_series[0].apply(lambda x : x.replace(u'\\xa0', \"_\"))\n\n# make it a set because O(1) check vs O(n) check\nbigram_series = set(bigram_series[0].to_list())","execution_count":null,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-90cf7bab-59d5-44f3-888e-e690aac4461c","deepnote_to_be_reexecuted":false,"source_hash":"3d29d77c","execution_millis":2,"execution_start":1613925973700,"deepnote_cell_type":"code"},"source":"class PreProcessor:\n    \n    def __init__(self, df:pd.DataFrame, words_to_remove:list, f_name:str, process_column:str='sentence',token:str=r'\\w+'):\n        \"\"\"df['sentence'] needs to be string \"\"\"\n        self.df = df\n        self.stop_words = set(stopwords.words('english')+words_to_remove)\n        self.f_name = f_name\n        self.process_column = process_column\n        self.tokenizer = RegexpTokenizer(token)\n        self.__did_process = False\n        self.exp_df = None\n    \n    def process(self) -> (pd.DataFrame, pd.DataFrame):\n        \"\"\"df['sentence'] is an array of tokenized words based on processing settings\n        \"\"\"\n        if self.__did_process:\n            return self.df, self.exp_df\n        \n        ###################    cleaning    ###########################\n        # add any other steps that you need. it will be applied to all dataset\n\n        self.df[self.process_column] = self.df[self.process_column].astype(str)\n\n        # remove \\n, \\r, \\t into \" \"\n        self.df[self.process_column] = self.df[self.process_column].apply(lambda x: x.replace('\\n', ' ').replace('\\r', '').replace('\\t', ' '))\n\n        # tokenize the strings into array. makes them lowercase too\n        self.df[self.process_column] = self.df[self.process_column].apply(lambda x: self.tokenizer.tokenize(x.lower()))\n\n        # compute 2 ngrams and check if they are in the list of ngrams\n        self.df[self.process_column] = self.df[self.process_column].apply(lambda x : x+['_'.join(i) for i in list(ngrams(x, 2)) if '_'.join(i) in bigram_series])\n        \n        # n-grams \n        # methodology: generate all ngrams then look if they are in this list http://phrasesinenglish.org/explorengrams.html#filterdiv\n        # https://albertauyeung.github.io/2018/06/03/generating-ngrams.html\n        # 'very' 'good'\n        # 'very_good' --> 2-ngrams , post\n        # 'very_bad' --> neg\n\n\n        # remove common stopwords from ntlk library and also some domain specific words\n        self.df[self.process_column] = self.df[self.process_column].apply(lambda x : [item for item in x if item not in self.stop_words])\n\n        #################################################################\n\n        self.df.to_csv(dataset_path.joinpath('{}_row_array_bigram.csv'.format(self.f_name)), index=False)\n\n        # see: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html\n        self.exp_df = self.df.explode(self.process_column)\n        self.exp_df.reset_index(inplace=True, drop=True)\n        self.exp_df.to_csv(dataset_path.joinpath('{}_exploded_bigram.csv'.format(self.f_name)), index=False)\n\n\n        self.__did_process = True\n        \n        return self.df, self.exp_df\n    \n    ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process IMDB Review\n* core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets\n* no more than 30 reviews are allowed for any given movie because reviews\n* negative review has a score less or equal to 4 out of 10\n* positive review has a score greater or equal to 7 out of 10\n* neutral ratings are not included in the train/test sets\n\nWithn CSV:\n* _review_id_ : id of review\n* _train_or_test_ : test or train, from dataset\n* _review_type_ : pos or neg\n* _review_number_ : rating given (out of 10)\n* _sentence_ : review sentece","metadata":{"tags":[],"cell_id":"00001-595b7b3f-4625-4e69-9128-390752d3fbcd","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-3ca96d2e-a230-408b-afc3-7f79c0e255be","deepnote_to_be_reexecuted":false,"source_hash":"d84a3ed8","execution_millis":2,"execution_start":1613925973704,"deepnote_cell_type":"code"},"source":"# dont need to run this anymore as everything is parsed\n\n# imdb_train_neg_path = Path('aclImdb','train', 'neg').glob('**/*')\n# imdb_train_pos_path = Path('aclImdb','train', 'pos').glob('**/*')\n\n# imdb_train_files = [x for x in imdb_train_neg_path if x.is_file()] + [x for x in imdb_train_pos_path if x.is_file()]\n\n# imdb_test_neg_path = Path('aclImdb','test', 'neg').glob('**/*')\n# imdb_test_pos_path = Path('aclImdb','test', 'pos').glob('**/*')\n\n# imdb_test_files = [x for x in imdb_test_neg_path if x.is_file()] + [x for x in imdb_test_pos_path if x.is_file()]","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert text file to CSV","metadata":{"tags":[],"cell_id":"00002-b38d32bb-5dac-4a62-b7e5-4b9755f3fe13","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We will put everything in one csv for now as it will make the cleaning easier. After we can simply use pandas to split the dataset into what it was.\n\n```\ndf[df['train_or_test] == 'test' ]\ndf[(df['train_or_test] == 'test') and (df['review_type] == 'pos') ]\n```\n\nThe arrays are converted into strings when saving to a csv. In order to convert them back run the following\n```\nfrom ast import literal_eval\ndf = pd.read_csv('imdb_no_punctuation.csv')\ndf['sentence'] = df['sentence'].apply(lambda s:literal_eval(s))\n```","metadata":{"tags":[],"cell_id":"00003-f383f657-414d-4a5e-8574-cf95cfa3e7b9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-b64dd319-d3b6-4fad-8fd3-6ab963962be1","deepnote_to_be_reexecuted":false,"source_hash":"ee259746","execution_millis":1,"output_cleared":false,"execution_start":1613925973753,"deepnote_cell_type":"code"},"source":"def convert_imbd_to_csv(file_lst, output_name):\n    df = pd.DataFrame(columns=['review_id','train_or_test','review_type', 'review_number' ,'sentence'])\n\n    for file in file_lst:\n        with open(file, 'r') as f:\n            detail = file.stem.split('_')\n            path = str(file.parent).split('/')\n            df = df.append({\n                'train_or_test':path[1],'review_type':path[2], 'sentence':f.read(), 'review_number':detail[1], 'review_id':detail[0]\n                }, ignore_index=True)\n    df.to_csv(output_name, index=False)\n\n# dont run this, it takes 10min\n# convert_imbd_to_csv(imdb_test_files+imdb_train_files, dataset_path.joinpath('imdb_raw.csv'))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-e066dd2b-e29b-41e9-a9b2-c65783062177","deepnote_to_be_reexecuted":false,"source_hash":"a474f01c","execution_millis":543,"execution_start":1613925973754,"deepnote_cell_type":"code"},"source":"imdb_raw_df = pd.read_csv(dataset_path.joinpath('imdb_raw.csv'))\n\"\"\"\nlist of words that are common to both dataset\n    > we can play with which word to remove and see the performance of the model\n    br is a html tag\n\"\"\"\ncommon_words = [\n    'br', \n    # 'film', \n    # 'movie', \n    # 'one', \n    # 'like', \n    # 'good',\n    # 'time'\n    ]\nimdb_processor = PreProcessor(imdb_raw_df, common_words,'imdb')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-d9b95416-94f8-4ede-bb43-6b837c8827a6","deepnote_to_be_reexecuted":false,"source_hash":"3df29f01","execution_millis":42274,"execution_start":1613925974312,"deepnote_cell_type":"code"},"source":"imdb_row_df,imdb_exploded_df = imdb_processor.process()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis","metadata":{"tags":[],"cell_id":"00011-6051ab0b-dd6d-4464-8355-1be46a4f0dd4","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-368dbb6c-c379-486c-b793-6aeaf8bb74b0","deepnote_to_be_reexecuted":false,"source_hash":"4dda9d44","execution_millis":1470,"execution_start":1613926017237,"deepnote_cell_type":"code"},"source":"grouped = imdb_exploded_df[imdb_exploded_df['review_type'] == 'pos']['sentence'].value_counts()\nprint(grouped[ grouped == 1].size)\n\n# we can finetune this, max is around 60k so we can remove up to like 500?. the better we can reduce this, the faster our model will be and the better the results\ngrouped[ grouped <= 500].size","execution_count":null,"outputs":[{"name":"stdout","text":"36796\n","output_type":"stream"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"145733"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-02331a35-3f42-49f5-aa5c-62115d31fe3a","deepnote_to_be_reexecuted":false,"source_hash":"72b3323b","execution_millis":1451,"execution_start":1613926019416,"deepnote_cell_type":"code"},"source":"imdb_exploded_df[imdb_exploded_df['review_type'] == 'pos']['sentence'].value_counts()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"film            42110\nof_the          41598\nmovie           37854\none             27320\nin_the          25738\n                ...  \nfewer_people        1\ntvms                1\ndouchewads          1\nblissed             1\ndispair             1\nName: sentence, Length: 147528, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Process 20 news group dataset from _sklearn_\n\nWithin CSV:\n* _sentence_ : from dataset\n* _target_ : from dataset\n* _train_or_test_ : train or test instance, from dataset\n\nThe data is pretty fucked from _fetch_20newsgroups_ so the CSV are tab separated, make sure to include `sep='\\t'` when you read the csv.","metadata":{"tags":[],"cell_id":"00013-b43b6b64-c54d-4a63-bf07-a0c08fea446d","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-d9ffacc8-56e7-4f45-8f17-23491dac05d3","deepnote_to_be_reexecuted":false,"source_hash":"43c862b7","execution_millis":9764,"execution_start":1613926020865,"deepnote_cell_type":"code"},"source":"from sklearn.datasets import fetch_20newsgroups\ntwenty_news_group_train  = fetch_20newsgroups(subset='train', remove=(['headers', 'footers', 'quotes']))\ntwenty_news_group_test  = fetch_20newsgroups(subset='test', remove=(['headers', 'footers', 'quotes']))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00015-5352ca9e-68ee-487b-b056-cd21a2a6b137","deepnote_to_be_reexecuted":false,"source_hash":"200fc6db","execution_millis":508,"output_cleared":false,"execution_start":1613926030642,"deepnote_cell_type":"code"},"source":"twenty_news_train_df = pd.DataFrame(data={'target': twenty_news_group_train.target, 'train_or_test':'train', 'sentence': twenty_news_group_train.data})\ntwenty_news_test_df = pd.DataFrame(data={'target': twenty_news_group_test.target, 'train_or_test':'test', 'sentence': twenty_news_group_test.data})\n\ntwenty_news_combined_df = twenty_news_train_df.append(twenty_news_test_df)\n\ntwenty_news_combined_df['sentence'] = twenty_news_combined_df['sentence'].apply(lambda x: x.replace('\\n', ' ').replace('\\r', '').replace('\\t', ' ').strip())\ntwenty_news_combined_df.reset_index(inplace=True)\ntwenty_news_combined_df.rename(columns={'index': 'id'}, inplace=True)\ntwenty_news_combined_df.to_csv(dataset_path.joinpath('twenty_news_raw.csv'), sep='\\t', index=False)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00017-651d9dad-b323-4706-8a55-db6c8ace61ba","deepnote_to_be_reexecuted":false,"source_hash":"d1139db1","execution_millis":189,"output_cleared":false,"execution_start":1613926031154,"deepnote_cell_type":"code"},"source":"twenty_news_raw_df = pd.read_csv(dataset_path.joinpath('twenty_news_raw.csv'), sep='\\t')\ncommon_words = [\n    # to be determined\n]\n\ntwenty_news_processor = PreProcessor(twenty_news_raw_df,common_words,'twenty_news')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00018-aa870949-2d66-4711-a03b-61cb36d855da","deepnote_to_be_reexecuted":false,"source_hash":"594482b","execution_millis":13293,"execution_start":1613926031352,"deepnote_cell_type":"code"},"source":"twenty_news_row_df,twenty_news_exploded_df = twenty_news_processor.process()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis of data","metadata":{"tags":[],"cell_id":"00018-d63f3bf0-3922-4a61-a462-d483d6d6f165","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f5e36ec1-5982-458d-86cb-de064c0212ca' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"fed49051-4d1f-44c3-9228-15b0ea7d7420","deepnote":{"is_reactive":false},"deepnote_execution_queue":[]}}